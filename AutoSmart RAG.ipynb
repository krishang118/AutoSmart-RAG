{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y keras\n",
    "!pip uninstall -y keras-nightly\n",
    "!pip uninstall -y keras-preprocessing\n",
    "!pip uninstall -y keras-vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "\n",
      "Execution Options:\n",
      "1. Interactive Mode\n",
      "2. Exit\n",
      "Selected choice: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:16:27,644 - INFO - Use pytorch device_name: mps\n",
      "2025-06-21 12:16:27,644 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-06-21 12:16:30,781 - INFO - Embedding model loaded on mps.\n",
      "2025-06-21 12:16:30,789 - INFO - ChromaDB initialized successfully.\n",
      "2025-06-21 12:16:30,799 - INFO - Testing model availability with direct generation...\n",
      "2025-06-21 12:16:35,010 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-21 12:16:35,038 - INFO - Model deepseek-r1:14b is working. Test response: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interactive mode...\n",
      "This is a RAG-based-Search-Augmented + Self-Correcting LLM Reasoning System.\n",
      "\n",
      "Enter your queries (type 'quit' to exit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:16:50,288 - INFO - Processing query: 'Did a plane crash in Ahmedabad?'.\n",
      "2025-06-21 12:16:50,291 - INFO - Web scraping...\n",
      "2025-06-21 12:16:50,292 - INFO - Web scraping...\n",
      "2025-06-21 12:16:50,304 - INFO - Loaded 9 results from cache.\n",
      "2025-06-21 12:16:50,304 - INFO - Chunking and embedding...\n",
      "2025-06-21 12:16:50,475 - INFO - Generated 7 chunks from https://www.bbc.com/news.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:16:50,619 - INFO - Generated 7 chunks from https://www.bbc.com/news/articles/c0l484l40gyo.\n",
      "2025-06-21 12:16:50,771 - INFO - Generated 5 chunks from https://www.nytimes.com.\n",
      "2025-06-21 12:16:50,999 - INFO - Generated 6 chunks from https://www.bbc.co.uk/news/articles/c0l484l40gyo.\n",
      "2025-06-21 12:16:51,200 - INFO - Generated 5 chunks from https://www.bbc.co.uk/news/articles/c5y5nq170z4o.\n",
      "2025-06-21 12:16:51,396 - INFO - Generated 6 chunks from https://www.bbc.co.uk/news/articles/cvgn23757jzo.\n",
      "2025-06-21 12:16:51,547 - INFO - Generated 7 chunks from https://www.bbc.co.uk/news/articles/clyzn0gjz5lo.\n",
      "2025-06-21 12:16:51,591 - INFO - Generated 4 chunks from https://abcnews.go.com.\n",
      "2025-06-21 12:16:51,709 - INFO - Generated 7 chunks from https://www.bbc.com/news/articles/ce818jlz5mlo.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f37804c1e6042c48df2563952295693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:16:52,452 - INFO - Storing in vector database...\n",
      "2025-06-21 12:16:52,589 - INFO - Added 54 unique documents to ChromaDB.\n",
      "2025-06-21 12:16:52,590 - INFO - Retrieving relevant context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72505d2f3c2949e488eb24972807cc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:16:52,730 - INFO - ChromaDB returned 5 documents.\n",
      "2025-06-21 12:16:52,731 - INFO - Document 0: distance=0.3376, similarity=0.6624, threshold=0.15\n",
      "2025-06-21 12:16:52,731 - INFO - Source: https://www.bbc.co.uk/news/articles/c5y5nq170z4o\n",
      "2025-06-21 12:16:52,731 - INFO - Title: What we know after Air India flight from Ahmedabad to London crashes\n",
      "2025-06-21 12:16:52,731 - INFO - Content preview: It was scheduled to land at London Gatwick at 18:25 BST. Moments after departing Ahmedabad, the plan...\n",
      "2025-06-21 12:16:52,732 - INFO - Document 0 included (similarity=0.6624).\n",
      "2025-06-21 12:16:52,732 - INFO - Document 1: distance=0.3376, similarity=0.6624, threshold=0.15\n",
      "2025-06-21 12:16:52,732 - INFO - Source: https://www.bbc.co.uk/news/articles/c5y5nq170z4o\n",
      "2025-06-21 12:16:52,732 - INFO - Title: What we know after Air India flight from Ahmedabad to London crashes\n",
      "2025-06-21 12:16:52,733 - INFO - Content preview: It was scheduled to land at London Gatwick at 18:25 BST. Moments after departing Ahmedabad, the plan...\n",
      "2025-06-21 12:16:52,733 - INFO - Document 1 included (similarity=0.6624).\n",
      "2025-06-21 12:16:52,733 - INFO - Document 2: distance=0.3376, similarity=0.6624, threshold=0.15\n",
      "2025-06-21 12:16:52,734 - INFO - Source: https://www.bbc.co.uk/news/articles/c5y5nq170z4o\n",
      "2025-06-21 12:16:52,734 - INFO - Title: What we know after Air India flight from Ahmedabad to London crashes\n",
      "2025-06-21 12:16:52,734 - INFO - Content preview: It was scheduled to land at London Gatwick at 18:25 BST. Moments after departing Ahmedabad, the plan...\n",
      "2025-06-21 12:16:52,734 - INFO - Document 2 included (similarity=0.6624).\n",
      "2025-06-21 12:16:52,735 - INFO - Document 3: distance=0.3376, similarity=0.6624, threshold=0.15\n",
      "2025-06-21 12:16:52,735 - INFO - Source: https://www.bbc.co.uk/news/articles/c5y5nq170z4o\n",
      "2025-06-21 12:16:52,735 - INFO - Title: What we know after Air India flight from Ahmedabad to London crashes\n",
      "2025-06-21 12:16:52,736 - INFO - Content preview: It was scheduled to land at London Gatwick at 18:25 BST. Moments after departing Ahmedabad, the plan...\n",
      "2025-06-21 12:16:52,736 - INFO - Document 3 included (similarity=0.6624).\n",
      "2025-06-21 12:16:52,736 - INFO - Document 4: distance=0.3376, similarity=0.6624, threshold=0.15\n",
      "2025-06-21 12:16:52,736 - INFO - Source: https://www.bbc.co.uk/news/articles/c5y5nq170z4o\n",
      "2025-06-21 12:16:52,737 - INFO - Title: What we know after Air India flight from Ahmedabad to London crashes\n",
      "2025-06-21 12:16:52,737 - INFO - Content preview: It was scheduled to land at London Gatwick at 18:25 BST. Moments after departing Ahmedabad, the plan...\n",
      "2025-06-21 12:16:52,737 - INFO - Document 4 included (similarity=0.6624).\n",
      "2025-06-21 12:16:52,738 - INFO - Returning 5 results after filtering.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1328 > 1024). Running this sequence through the model will result in indexing errors\n",
      "2025-06-21 12:16:52,745 - INFO - Performing initial reasoning...\n",
      "2025-06-21 12:17:39,741 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-21 12:17:39,763 - INFO - LLM generation successful on attempt 1.\n",
      "2025-06-21 12:17:39,763 - INFO - Self-correcting...\n",
      "2025-06-21 12:18:53,497 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-21 12:18:53,526 - INFO - LLM generation successful on attempt 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Processing Time: 123.24 seconds.\n",
      "Pages Scraped: 9\n",
      "Text Chunks: 54\n",
      "Retrieved Chunks: 5\n",
      "Similarity Scores: ['0.662', '0.662', '0.662']\n",
      "\n",
      "Final Answer:\n",
      "Yes, a plane crash occurred in Ahmedabad. The incident took place moments after departure when the aircraft lost altitude and crashed into Meghani Nagar, a residential area. The flight was scheduled to land at London Gatwick but crashed shortly after takeoff. The plane struck a doctors' hostel at Byramjee Jeejeebhoy Medical College and Civil Hospital on Thursday during lunch break. Parts of the plane impacted the dining hall's roof, leaving abandoned tables and plates from the mealtime scattered in the canteen.\n",
      "\n",
      "Flight tracking data showed the signal was lost less than a minute after takeoff at an altitude of 625ft (190m), with no response to the mayday call. The crash resulted in casualties, as reported by the BBC News.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:19:16,977 - INFO - Processing query: 'What's going on with Trump and Musk?'.\n",
      "2025-06-21 12:19:16,978 - INFO - Web scraping...\n",
      "2025-06-21 12:19:16,979 - INFO - Web scraping...\n",
      "2025-06-21 12:19:16,996 - INFO - Loaded 5 results from cache.\n",
      "2025-06-21 12:19:16,996 - INFO - Chunking and embedding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:19:17,201 - INFO - Generated 5 chunks from https://www.nytimes.com.\n",
      "2025-06-21 12:19:17,300 - INFO - Generated 7 chunks from https://www.bbc.com/news.\n",
      "2025-06-21 12:19:17,342 - INFO - Generated 4 chunks from https://abcnews.go.com.\n",
      "2025-06-21 12:19:17,360 - INFO - Generated 1 chunks from https://www.bbc.co.uk/news/videos/c8d18dm967qo.\n",
      "2025-06-21 12:19:17,516 - INFO - Generated 5 chunks from https://www.bbc.co.uk/news/articles/ceqgdnd2g9xo.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7474b92fee54f6bb59213fcdf6b604d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:19:18,304 - INFO - Storing in vector database...\n",
      "2025-06-21 12:19:18,391 - INFO - Added 22 unique documents to ChromaDB.\n",
      "2025-06-21 12:19:18,392 - INFO - Retrieving relevant context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7891dc4a488746278317d7c62d353c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:19:18,486 - INFO - ChromaDB returned 5 documents.\n",
      "2025-06-21 12:19:18,487 - INFO - Document 0: distance=0.3432, similarity=0.6568, threshold=0.15\n",
      "2025-06-21 12:19:18,487 - INFO - Source: https://www.bbc.co.uk/news/articles/ceqgdnd2g9xo\n",
      "2025-06-21 12:19:18,487 - INFO - Title: Will Musk's explosive row with Trump help or harm his businesses?\n",
      "2025-06-21 12:19:18,488 - INFO - Content preview: Register Sign In Home News Sport Business Innovation Culture Arts Travel Earth Audio Video Live ADVE...\n",
      "2025-06-21 12:19:18,488 - INFO - Document 0 included (similarity=0.6568).\n",
      "2025-06-21 12:19:18,488 - INFO - Document 1: distance=0.3432, similarity=0.6568, threshold=0.15\n",
      "2025-06-21 12:19:18,494 - INFO - Source: https://www.bbc.co.uk/news/articles/ceqgdnd2g9xo\n",
      "2025-06-21 12:19:18,522 - INFO - Title: Will Musk's explosive row with Trump help or harm his businesses?\n",
      "2025-06-21 12:19:18,523 - INFO - Content preview: Register Sign In Home News Sport Business Innovation Culture Arts Travel Earth Audio Video Live ADVE...\n",
      "2025-06-21 12:19:18,523 - INFO - Document 1 included (similarity=0.6568).\n",
      "2025-06-21 12:19:18,523 - INFO - Document 2: distance=0.3488, similarity=0.6512, threshold=0.15\n",
      "2025-06-21 12:19:18,523 - INFO - Source: https://www.bbc.co.uk/news/articles/ceqgdnd2g9xo\n",
      "2025-06-21 12:19:18,523 - INFO - Title: Will Musk's explosive row with Trump help or harm his businesses?\n",
      "2025-06-21 12:19:18,524 - INFO - Content preview: And theres something else to factor in too: Musks own motivation. The talk in Silicon Valley lately ...\n",
      "2025-06-21 12:19:18,524 - INFO - Document 2 included (similarity=0.6512).\n",
      "2025-06-21 12:19:18,524 - INFO - Document 3: distance=0.3488, similarity=0.6512, threshold=0.15\n",
      "2025-06-21 12:19:18,525 - INFO - Source: https://www.bbc.co.uk/news/articles/ceqgdnd2g9xo\n",
      "2025-06-21 12:19:18,525 - INFO - Title: Will Musk's explosive row with Trump help or harm his businesses?\n",
      "2025-06-21 12:19:18,526 - INFO - Content preview: And theres something else to factor in too: Musks own motivation. The talk in Silicon Valley lately ...\n",
      "2025-06-21 12:19:18,526 - INFO - Document 3 included (similarity=0.6512).\n",
      "2025-06-21 12:19:18,526 - INFO - Document 4: distance=0.4303, similarity=0.5697, threshold=0.15\n",
      "2025-06-21 12:19:18,527 - INFO - Source: https://www.bbc.co.uk/news/articles/ceqgdnd2g9xo\n",
      "2025-06-21 12:19:18,527 - INFO - Title: Will Musk's explosive row with Trump help or harm his businesses?\n",
      "2025-06-21 12:19:18,528 - INFO - Content preview: They rebounded a little on Friday following some indications tempers were cooling. Even so, for the ...\n",
      "2025-06-21 12:19:18,528 - INFO - Document 4 included (similarity=0.5697).\n",
      "2025-06-21 12:19:18,528 - INFO - Returning 5 results after filtering.\n",
      "2025-06-21 12:19:18,538 - INFO - Performing initial reasoning...\n",
      "2025-06-21 12:20:37,079 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-21 12:20:37,121 - INFO - LLM generation successful on attempt 1.\n",
      "2025-06-21 12:20:37,124 - INFO - Self-correcting...\n",
      "2025-06-21 12:22:17,645 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-21 12:22:17,665 - INFO - LLM generation successful on attempt 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Processing Time: 180.69 seconds.\n",
      "Pages Scraped: 5\n",
      "Text Chunks: 22\n",
      "Retrieved Chunks: 5\n",
      "Similarity Scores: ['0.657', '0.657', '0.651']\n",
      "\n",
      "Final Answer:\n",
      "The relationship between Elon Musk and Donald Trump has recently been marked by significant public disputes that have impacted both their personal reputations and business interests. Here's an organized summary of the situation:\n",
      "\n",
      "1. **Public Dispute**: A heated social media argument between Musk and Trump escalated, with comments from Musk about the White House leading to a high-profile feud.\n",
      "\n",
      "2. **Impact on Tesla**: The conflict resulted in a 14% drop in Tesla's stock price during trading hours on June 7, 2025. Although there was a slight recovery as tensions eased, investor confidence remained shaken.\n",
      "\n",
      "3. **Investor Concerns**: Investors, including long-time supporters like Ross Gerber, have expressed doubts about Musk's focus on his businesses. Some have reduced their holdings in Tesla due to concerns over Musk's political involvement and its potential impact on company performance. Gerber described the situation as \"extremely painful\" and criticized Musk's actions.\n",
      "\n",
      "4. **Focus and Motivation**: Questions have been raised about whether Musk is still fully committed to running his companies effectively, particularly concerning Tesla's lag in autonomous vehicle technology compared to competitors like Waymo. This shift in priorities has led to investor skepticism.\n",
      "\n",
      "5. **Broader Issues**: The feud with Trump is seen as a symptom of deeper issues within Tesla, including its struggle to compete in the autonomous vehicle market and keep up with production targets. Musk is scheduled to oversee the launch of robo-taxis in Austin, Texas, highlighting the specific challenges faced by Tesla.\n",
      "\n",
      "In summary, while the public dispute between Musk and Trump has captured attention, it reflects underlying concerns about Musk's business priorities and Tesla's competitive position in the tech industry.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote_plus\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import ollama\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shutil\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "@dataclass\n",
    "class Config:\n",
    "    embedding_model_name: str = \"all-MiniLM-L6-v2\"\n",
    "    llm_model_name: str = \"deepseek-r1:14b\"\n",
    "    max_tokens: int = 4096\n",
    "    temperature: float = 0.7\n",
    "    chunk_size: int = 256\n",
    "    chunk_overlap: int = 50\n",
    "    max_chunks_per_doc: int = 20\n",
    "    top_k_retrieval: int = 5\n",
    "    similarity_threshold: float = 0.15 \n",
    "    max_pages_per_query: int = 5  \n",
    "    request_timeout: int = 10\n",
    "    selenium_timeout: int = 15\n",
    "    max_text_length: int = 5000\n",
    "    cache_dir: str = \"./scrape_cache\"\n",
    "    cache_expiry_hours: int = 48  \n",
    "    vector_db_type: str = \"chroma\"\n",
    "    persist_directory: str = \"./chroma_db\"\n",
    "    device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    batch_size: int = 64\n",
    "config = Config()\n",
    "class WebScraper:    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.driver = None\n",
    "        os.makedirs(config.cache_dir, exist_ok=True)\n",
    "    def setup_selenium(self):\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            logger.info(\"Selenium WebDriver initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Selenium: {e}.\")\n",
    "    def scrape_with_requests(self, url: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=config.request_timeout)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            title = soup.find('title')\n",
    "            title_text = title.get_text().strip() if title else \"\"\n",
    "            content = soup.get_text(separator=' ', strip=True)[:config.max_text_length]\n",
    "            if len(content) < 500:\n",
    "                return None\n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title_text,\n",
    "                'content': content,\n",
    "                'method': 'requests',\n",
    "                'timestamp': datetime.now().isoformat()}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url} with requests: {e}.\")\n",
    "            return None\n",
    "    async def scrape_with_selenium(self, url: str) -> Dict[str, Any]:\n",
    "        if not self.driver:\n",
    "            self.setup_selenium()\n",
    "        if not self.driver:\n",
    "            return None\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            WebDriverWait(self.driver, config.selenium_timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            time.sleep(2)\n",
    "            title = self.driver.title\n",
    "            content = self.driver.find_element(By.TAG_NAME, \"body\").text[:config.max_text_length]\n",
    "            if len(content) < 500:\n",
    "                return None\n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'method': 'selenium',\n",
    "                'timestamp': datetime.now().isoformat()}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url} with Selenium: {e}.\")\n",
    "            return None\n",
    "    async def scrape_bbc_search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        if not self.driver:\n",
    "            self.setup_selenium()\n",
    "        if not self.driver:\n",
    "            return []\n",
    "        try:\n",
    "            search_url = f\"https://www.bbc.co.uk/search?q={quote_plus(query)}\"\n",
    "            self.driver.get(search_url)\n",
    "            WebDriverWait(self.driver, config.selenium_timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            urls = []\n",
    "            for a in soup.select(\"a[href^='https://www.bbc.']\"):\n",
    "                href = a['href']\n",
    "                if '/news/' in href and href not in urls:\n",
    "                    urls.append(href)\n",
    "            urls = urls[:config.max_pages_per_query]\n",
    "            results = []\n",
    "            for url in urls:\n",
    "                result = await self.scrape_with_selenium(url)\n",
    "                if result and result.get('content'):\n",
    "                    results.append(result)\n",
    "                    logger.info(f\"Scraped BBC News {url}: {len(result['content'])} chars\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping BBC News search: {e}.\")\n",
    "            return []\n",
    "    async def search_and_scrape(self, query: str) -> List[Dict[str, Any]]:\n",
    "        logger.info(\"Web scraping...\")\n",
    "        results = []        \n",
    "        cache_key = hashlib.md5(query.encode()).hexdigest()\n",
    "        cache_path = os.path.join(config.cache_dir, f\"{cache_key}.pkl\")\n",
    "        if os.path.exists(cache_path):\n",
    "            cache_age = (time.time() - os.path.getmtime(cache_path)) / 3600\n",
    "            if cache_age < config.cache_expiry_hours:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    cached_results = pickle.load(f)\n",
    "                if all(len(r.get('content', '')) >= 500 for r in cached_results):\n",
    "                    logger.info(f\"Loaded {len(cached_results)} results from cache.\")\n",
    "                    return cached_results\n",
    "        priority_urls = [\n",
    "            \"https://www.bbc.com/news\",\n",
    "            \"https://abcnews.go.com\",\n",
    "            \"https://www.nytimes.com\"\n",
    "        ][:config.max_pages_per_query]\n",
    "        search_query = f\"{quote_plus(query)} site:*.news | site:bbc.com | site:abcnews.go.com | site:nytimes.com\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_query}&num={config.max_pages_per_query}\"\n",
    "        urls = []\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(search_url, headers=self.session.headers, timeout=config.request_timeout) as response:\n",
    "                    response.raise_for_status()\n",
    "                    soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                    for a in soup.select(\"a[href^='http']\"):\n",
    "                        href = a['href']\n",
    "                        if any(keyword in href for keyword in ['.news', 'articles', 'post']) and 'google.com' not in href:\n",
    "                            urls.append(href)\n",
    "                    urls = list(dict.fromkeys(urls))[:config.max_pages_per_query]\n",
    "                    logger.info(f\"Found {len(urls)} relevant URLs from Google search.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching Google search results: {e}.\")        \n",
    "        all_urls = priority_urls + [u for u in urls if u not in priority_urls][:config.max_pages_per_query - len(priority_urls)]        \n",
    "        async def scrape_url(url):\n",
    "            for attempt in range(5):\n",
    "                result = self.scrape_with_requests(url)\n",
    "                if result and len(result.get('content', '')) >= 500:\n",
    "                    return result\n",
    "                logger.warning(f\"Attempt {attempt+1} failed for {url} with requests, trying Selenium...\")\n",
    "                result = await self.scrape_with_selenium(url)\n",
    "                if result and len(result.get('content', '')) >= 500:\n",
    "                    return result\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "            return None\n",
    "        tasks = [scrape_url(url) for url in all_urls]\n",
    "        tasks.append(self.scrape_bbc_search(query))\n",
    "        results = []\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            result = await task\n",
    "            if isinstance(result, list):  \n",
    "                results.extend(result)\n",
    "            elif result:\n",
    "                results.append(result)        \n",
    "        if results:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "            logger.info(f\"Cached {len(results)} results for query: {query}\")\n",
    "        return results[:config.max_pages_per_query]\n",
    "    def cleanup(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "class TextChunker:    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load tokenizer: {e}. Using basic word counting.\")\n",
    "            self.tokenizer = None\n",
    "    def chunk_text(self, text: str, metadata: Dict = None) -> List[Dict[str, Any]]:\n",
    "        text = self._clean_text(text)        \n",
    "        if not metadata:\n",
    "            metadata = {'source': 'unknown', 'created_at': datetime.now().isoformat()}\n",
    "        if 'source' not in metadata:\n",
    "            metadata['source'] = 'unknown'\n",
    "        if 'created_at' not in metadata:\n",
    "            metadata['created_at'] = datetime.now().isoformat()\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        for sentence in sentences:\n",
    "            if self.tokenizer:\n",
    "                try:\n",
    "                    sentence_length = len(self.tokenizer.encode(sentence))\n",
    "                except:\n",
    "                    sentence_length = len(sentence.split())\n",
    "            else:\n",
    "                sentence_length = len(sentence.split())\n",
    "            if current_length + sentence_length > config.chunk_size and current_chunk:\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'token_count': current_length,\n",
    "                    'metadata': metadata.copy()})\n",
    "                overlap_sentences = current_chunk[-2:] if len(current_chunk) >= 2 else current_chunk\n",
    "                current_chunk = overlap_sentences + [sentence]\n",
    "                if self.tokenizer:\n",
    "                    try:\n",
    "                        current_length = sum(len(self.tokenizer.encode(s)) for s in current_chunk)\n",
    "                    except:\n",
    "                        current_length = sum(len(s.split()) for s in current_chunk)\n",
    "                else:\n",
    "                    current_length = sum(len(s.split()) for s in current_chunk)\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_length,\n",
    "                'metadata': metadata.copy()})\n",
    "        return chunks[:config.max_chunks_per_doc]\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', '', text)\n",
    "        return text.strip()\n",
    "class EmbeddingManager:    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(config.embedding_model_name)\n",
    "            if config.device == \"mps\":\n",
    "                self.model = self.model.to(\"mps\")\n",
    "            logger.info(f\"Embedding model loaded on {config.device}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load embedding model: {e}.\")\n",
    "            raise\n",
    "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        try:\n",
    "            non_empty_texts = [text for text in texts if text.strip()]\n",
    "            if not non_empty_texts:\n",
    "                return np.array([])\n",
    "            embeddings = self.model.encode(\n",
    "                non_empty_texts,\n",
    "                batch_size=config.batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {e}.\")\n",
    "            return np.array([])\n",
    "    def embed_query(self, query: str) -> np.ndarray:\n",
    "        if not query.strip():\n",
    "            return np.array([])\n",
    "        return self.model.encode([query])[0]\n",
    "class VectorDatabase:    \n",
    "    def __init__(self, db_type: str = \"chroma\"):\n",
    "        self.db_type = db_type\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        if db_type == \"faiss\":\n",
    "            self.index = None\n",
    "        elif db_type == \"chroma\":\n",
    "            try:\n",
    "                os.makedirs(config.persist_directory, exist_ok=True)\n",
    "                if not os.access(config.persist_directory, os.W_OK):\n",
    "                    raise PermissionError(f\"Cannot write to {config.persist_directory}.\")\n",
    "                self.client = chromadb.PersistentClient(path=config.persist_directory)\n",
    "                self.collection = self.client.get_or_create_collection(\n",
    "                    name=\"rag_collection\",\n",
    "                    metadata={\"hnsw:space\": \"cosine\"}\n",
    "                )\n",
    "                logger.info(\"ChromaDB initialized successfully.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize ChromaDB: {e}.\")\n",
    "                raise\n",
    "    def add_documents(self, chunks: List[Dict[str, Any]], embeddings: np.ndarray):\n",
    "        if len(chunks) == 0 or len(embeddings) == 0:\n",
    "            logger.warning(\"No chunks or embeddings to add.\")\n",
    "            return\n",
    "        if self.db_type == \"faiss\":\n",
    "            self._add_to_faiss(chunks, embeddings)\n",
    "        elif self.db_type == \"chroma\":\n",
    "            self._add_to_chroma(chunks, embeddings)\n",
    "    def _add_to_faiss(self, chunks: List[Dict[str, Any]], embeddings: np.ndarray):\n",
    "        if self.index is None:\n",
    "            dimension = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dimension)            \n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        self.chunks.extend(chunks)\n",
    "    def _add_to_chroma(self, chunks: List[Dict[str, Any]], embeddings: np.ndarray):\n",
    "        try:\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            ids = []\n",
    "            valid_embeddings = []\n",
    "            seen_texts = set()            \n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                if not chunk.get('text', '').strip():\n",
    "                    continue\n",
    "                text_hash = hashlib.md5(chunk['text'].encode()).hexdigest()\n",
    "                if text_hash in seen_texts:\n",
    "                    logger.info(f\"Skipping duplicate chunk {i} with text: {chunk['text'][:50]}...\")\n",
    "                    continue\n",
    "                seen_texts.add(text_hash)\n",
    "                metadata = chunk.get('metadata', {})\n",
    "                if not metadata or not isinstance(metadata, dict):\n",
    "                    metadata = {\n",
    "                        'source': 'unknown',\n",
    "                        'created_at': datetime.now().isoformat(),\n",
    "                        'chunk_index': i}\n",
    "                clean_metadata = {}\n",
    "                for key, value in metadata.items():\n",
    "                    if value is not None:\n",
    "                        clean_metadata[str(key)] = str(value)\n",
    "                if 'source' not in clean_metadata:\n",
    "                    clean_metadata['source'] = 'unknown'\n",
    "                if 'created_at' not in clean_metadata:\n",
    "                    clean_metadata['created_at'] = datetime.now().isoformat()\n",
    "                documents.append(chunk['text'])\n",
    "                metadatas.append(clean_metadata)\n",
    "                ids.append(f\"doc_{int(time.time())}_{i}_{text_hash[:8]}\")\n",
    "                valid_embeddings.append(embedding.tolist())\n",
    "            if documents:\n",
    "                self.collection.add(\n",
    "                    documents=documents,\n",
    "                    embeddings=valid_embeddings,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids)\n",
    "                logger.info(f\"Added {len(documents)} unique documents to ChromaDB.\")\n",
    "            else:\n",
    "                logger.warning(\"No valid documents to add to ChromaDB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding documents to ChromaDB: {e}.\")\n",
    "            raise\n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        if len(query_embedding) == 0:\n",
    "            return []            \n",
    "        if self.db_type == \"faiss\":\n",
    "            return self._search_faiss(query_embedding, k)\n",
    "        elif self.db_type == \"chroma\":\n",
    "            return self._search_chroma(query_embedding, k)\n",
    "    def _search_faiss(self, query_embedding: np.ndarray, k: int) -> List[Dict[str, Any]]:\n",
    "        if self.index is None or len(self.chunks) == 0:\n",
    "            return []            \n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks) and score >= config.similarity_threshold:\n",
    "                result = self.chunks[idx].copy()\n",
    "                result['score'] = float(score)\n",
    "                results.append(result)\n",
    "        return results\n",
    "    def _search_chroma(self, query_embedding: np.ndarray, k: int) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=k\n",
    "            )            \n",
    "            formatted_results = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                logger.info(f\"ChromaDB returned {len(results['documents'][0])} documents.\")\n",
    "                for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                    results['documents'][0],\n",
    "                    results['metadatas'][0] if results['metadatas'] else [{}] * len(results['documents'][0]),\n",
    "                    results['distances'][0] if results['distances'] else [0.0] * len(results['documents'][0])\n",
    "                )):\n",
    "                    similarity_score = 1.0 - distance\n",
    "                    logger.info(f\"Document {i}: distance={distance:.4f}, similarity={similarity_score:.4f}, threshold={config.similarity_threshold}\")\n",
    "                    logger.info(f\"Source: {metadata.get('url', metadata.get('source', 'Unknown'))}\")\n",
    "                    logger.info(f\"Title: {metadata.get('title', 'Unknown')}\")\n",
    "                    logger.info(f\"Content preview: {doc[:100]}...\")\n",
    "                    if similarity_score >= config.similarity_threshold:\n",
    "                        formatted_results.append({\n",
    "                            'text': doc,\n",
    "                            'metadata': metadata if metadata else {},\n",
    "                            'score': similarity_score,\n",
    "                            'distance': distance\n",
    "                        })\n",
    "                        logger.info(f\"Document {i} included (similarity={similarity_score:.4f}).\")\n",
    "                    else:\n",
    "                        logger.info(f\"Document {i} filtered out (similarity={similarity_score:.4f} < threshold={config.similarity_threshold}).\")\n",
    "            else:\n",
    "                logger.warning(\"No documents returned from ChromaDB query.\")\n",
    "            logger.info(f\"Returning {len(formatted_results)} results after filtering.\")\n",
    "            return formatted_results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching ChromaDB: {e}.\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "class DeepSeekLLM:    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.client = ollama.Client()\n",
    "            self.model_available = self._check_model_availability()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Ollama client: {e}.\")\n",
    "            raise\n",
    "    def _check_model_availability(self) -> bool:\n",
    "        model_name = config.llm_model_name\n",
    "        logger.info(\"Testing model availability with direct generation...\")\n",
    "        try:\n",
    "            test_response = self.client.chat(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                options={\"num_predict\": 10})\n",
    "            if test_response and 'message' in test_response and test_response['message'].get('content'):\n",
    "                logger.info(f\"Model {model_name} is working. Test response: {test_response['message']['content'][:50]}...\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Direct generation test failed: {e}.\")\n",
    "        try:\n",
    "            response = self.client.list()\n",
    "            available_models = []\n",
    "            if isinstance(response, dict) and 'models' in response:\n",
    "                for model in response.get('models', []):\n",
    "                    model_name_field = model.get('name', model.get('model', '')).strip()\n",
    "                    if model_name_field:\n",
    "                        available_models.append(model_name_field)\n",
    "                        if ':' in model_name_field:\n",
    "                            base_name = model_name_field.split(':')[0]\n",
    "                            available_models.append(base_name)\n",
    "            model_variants = [\n",
    "                model_name,\n",
    "                model_name.lower(),\n",
    "                model_name.replace('-', '_'),\n",
    "                model_name.split(':')[0] if ':' in model_name else model_name,\n",
    "                f\"deepseek-r1\",\n",
    "                f\"deepseek-r1:latest\",\n",
    "                f\"deepseek-r1:14b-instruct\"]\n",
    "            for variant in model_variants:\n",
    "                if variant in available_models:\n",
    "                    logger.info(f\"Found model variant: {variant}.\")\n",
    "                    config.llm_model_name = variant\n",
    "                    return True\n",
    "            logger.warning(f\"Model not found in list. Available: {available_models}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to check model list: {e}.\")\n",
    "        logger.info(f\"Cannot verify model {model_name}, but will attempt to use...\")\n",
    "        return True\n",
    "    def generate(self, prompt: str, system_message: str = None) -> str:\n",
    "        from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "        def _generate():\n",
    "            messages = []\n",
    "            if system_message:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            response = self.client.chat(\n",
    "                model=config.llm_model_name,\n",
    "                messages=messages,\n",
    "                options={\n",
    "                    \"temperature\": config.temperature,\n",
    "                    \"num_predict\": config.max_tokens,})\n",
    "            if response and 'message' in response:\n",
    "                content = response['message']['content']\n",
    "                if '<think>' in content and '</think>' in content:\n",
    "                    parts = content.split('</think>')\n",
    "                    if len(parts) > 1:\n",
    "                        content = parts[-1].strip()\n",
    "                    else:\n",
    "                        thinking_content = content.split('<think>')[1].split('</think>')[0] if '<think>' in content else content\n",
    "                        content = thinking_content.strip()\n",
    "                content = content.replace('<think>', '').replace('</think>', '').strip()\n",
    "                if not content:\n",
    "                    content = response['message']['content']\n",
    "                return content if content else \"No response generated by the model.\"    \n",
    "            return \"Error: Unexpected response format from model.\"\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                    future = executor.submit(_generate)\n",
    "                    result = future.result(timeout=180)  \n",
    "                logger.info(f\"LLM generation successful on attempt {attempt+1}.\")\n",
    "                return result\n",
    "            except TimeoutError:\n",
    "                logger.warning(f\"LLM generation timed out on attempt {attempt+1}.\")\n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if \"model\" in error_msg and \"not found\" in error_msg:\n",
    "                    return f\"Error: Model '{config.llm_model_name}' not found. Please run: ollama pull {config.llm_model_name}.\"\n",
    "                elif \"connection\" in error_msg or \"refused\" in error_msg:\n",
    "                    return f\"Error: Cannot connect to Ollama. Please ensure Ollama is running using 'ollama serve'.\"\n",
    "                logger.error(f\"LLM generation failed on attempt {attempt+1}: {e}.\")\n",
    "            time.sleep(2 ** attempt)\n",
    "        return \"Error: LLM generation failed after 5 attempts.\"\n",
    "class RAGReasoner:    \n",
    "    def __init__(self):\n",
    "        self.scraper = WebScraper()\n",
    "        self.chunker = TextChunker()\n",
    "        self.embedder = EmbeddingManager()\n",
    "        self.vector_db = VectorDatabase(config.vector_db_type)\n",
    "        self.llm = DeepSeekLLM()\n",
    "    async def process_query(self, query: str, use_web_search: bool = True) -> Dict[str, Any]:\n",
    "        logger.info(f\"Processing query: '{query}'.\")        \n",
    "        results = {\n",
    "            'query': query,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'steps': {}}\n",
    "        try:\n",
    "            all_chunks = []\n",
    "            if use_web_search:\n",
    "                logger.info(\"Web scraping...\")\n",
    "                scraped_data = await self.scraper.search_and_scrape(query)\n",
    "                results['steps']['scraping'] = {\n",
    "                    'num_pages': len(scraped_data),\n",
    "                    'pages': [{'url': d['url'], 'title': d['title']} for d in scraped_data]\n",
    "                }\n",
    "                logger.info(\"Chunking and embedding...\")\n",
    "                for data in scraped_data:\n",
    "                    if data and data.get('content'):\n",
    "                        chunks = self.chunker.chunk_text(\n",
    "                            data['content'],\n",
    "                            metadata={\n",
    "                                'url': data['url'],\n",
    "                                'title': data['title'],\n",
    "                                'scrape_method': data['method'],\n",
    "                                'timestamp': data['timestamp']\n",
    "                            })\n",
    "                        all_chunks.extend(chunks)\n",
    "                        logger.info(f\"Generated {len(chunks)} chunks from {data['url']}.\")\n",
    "                if all_chunks:\n",
    "                    texts = [chunk['text'] for chunk in all_chunks]\n",
    "                    embeddings = self.embedder.embed_texts(texts)\n",
    "                    if len(embeddings) > 0:\n",
    "                        logger.info(\"Storing in vector database...\")\n",
    "                        self.vector_db.add_documents(all_chunks, embeddings)\n",
    "                        results['steps']['embedding'] = {\n",
    "                            'num_chunks': len(all_chunks),\n",
    "                            'embedding_dimension': embeddings.shape[1]}\n",
    "                    else:\n",
    "                        logger.warning(\"Failed to generate embeddings.\")\n",
    "                else:\n",
    "                    logger.warning(\"No chunks generated from scraped data.\")\n",
    "            else:\n",
    "                logger.info(\"Skipping web search, using existing vector database.\")\n",
    "            logger.info(\"Retrieving relevant context...\")\n",
    "            query_embedding = self.embedder.embed_query(query)\n",
    "            if len(query_embedding) > 0:\n",
    "                relevant_chunks = self.vector_db.search(query_embedding, config.top_k_retrieval)\n",
    "            else:\n",
    "                relevant_chunks = []\n",
    "            x_context = \"\"            \n",
    "            if not relevant_chunks:\n",
    "                logger.warning(\"No relevant chunks found in vector database.\")\n",
    "                context = f\"I don't have specific information about '{query}' in my current knowledge base. Let me provide a general response based on my training.\\n\\n{x_context}\"\n",
    "                results['steps']['retrieval'] = {\n",
    "                    'num_retrieved': 0,\n",
    "                    'scores': [],\n",
    "                    'note': 'No relevant chunks found.'}\n",
    "            else:\n",
    "                context = \"\\n\\n\".join([\n",
    "                    f\"Source: {chunk.get('metadata', {}).get('url', chunk.get('metadata', {}).get('source', 'Unknown'))}\\n{chunk['text']}\"\n",
    "                    for chunk in relevant_chunks\n",
    "                ])\n",
    "                if self.chunker.tokenizer:\n",
    "                    context_tokens = len(self.chunker.tokenizer.encode(context))\n",
    "                    if context_tokens > 3000:\n",
    "                        context = self.chunker.tokenizer.decode(self.chunker.tokenizer.encode(context)[:3000])\n",
    "                context += f\"\\n\\n{x_context}\"\n",
    "                results['steps']['retrieval'] = {\n",
    "                    'num_retrieved': len(relevant_chunks),\n",
    "                    'scores': [chunk['score'] for chunk in relevant_chunks]\n",
    "                }\n",
    "            logger.info(\"Performing initial reasoning...\")\n",
    "            reasoning_prompt = self._create_reasoning_prompt(query, context)\n",
    "            initial_answer = self.llm.generate(reasoning_prompt)\n",
    "            results['steps']['initial_reasoning'] = {\n",
    "                'answer': initial_answer\n",
    "            }\n",
    "            logger.info(\"Self-correcting...\")\n",
    "            correction_prompt = self._create_correction_prompt(query, context, initial_answer)\n",
    "            final_answer = self.llm.generate(correction_prompt)\n",
    "            results['steps']['self_correction'] = {\n",
    "                'answer': final_answer\n",
    "            }\n",
    "            results['final_answer'] = final_answer\n",
    "            results['context_used'] = context\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in processing pipeline: {e}.\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results['error'] = str(e)\n",
    "            results['final_answer'] = f\"Error processing query: {str(e)}\"\n",
    "        finally:\n",
    "            self.scraper.cleanup()\n",
    "        return results\n",
    "    def _create_reasoning_prompt(self, query: str, context: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are an expert AI assistant with access to relevant information. Your task is to provide a comprehensive and accurate answer to the user's question using the provided context.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the provided context carefully, prioritizing verified news sources over unverified social media posts\n",
    "2. Identify key information relevant to the question\n",
    "3. Reason through the problem step by step\n",
    "4. Provide a well-structured, comprehensive answer\n",
    "5. If the context doesn't contain sufficient information, clearly state what's missing\n",
    "6. Cite sources when possible, noting if information comes from unverified social media\n",
    "\n",
    "Please provide your answer directly without any XML tags or special formatting:\"\"\"\n",
    "    def _create_correction_prompt(self, query: str, context: str, initial_answer: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are an expert AI assistant performing self-correction on a previous response. Your task is to review, analyze, and improve the initial answer.\n",
    "\n",
    "Original Question: {query}\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Initial Answer:\n",
    "{initial_answer}\n",
    "\n",
    "Self-Correction Instructions:\n",
    "1. Critically evaluate the initial answer for:\n",
    "   - Factual accuracy, especially verifying against news sources\n",
    "   - Completeness\n",
    "   - Logical consistency\n",
    "   - Proper use of context\n",
    "   - Clarity and structure\n",
    "\n",
    "2. Check if the answer:\n",
    "   - Fully addresses the question\n",
    "   - Prioritizes verified news sources over unverified social media\n",
    "   - Contains any contradictions or errors\n",
    "   - Could be improved in any way\n",
    "\n",
    "3. Provide an improved final answer that:\n",
    "   - Corrects any identified errors\n",
    "   - Adds missing important information\n",
    "   - Improves clarity and structure\n",
    "   - Better utilizes the available context\n",
    "\n",
    "Provide your final corrected answer directly without any XML tags or special formatting:   \n",
    "\"\"\"\n",
    "class RAGSystem:    \n",
    "    def __init__(self):\n",
    "        self.reasoner = RAGReasoner()\n",
    "    async def run_interactive(self):\n",
    "        print(\"This is a RAG-based-Search-Augmented + Self-Correcting LLM Reasoning System.\\n\")\n",
    "        print(\"Enter your queries (type 'quit' to exit).\")\n",
    "        while True:\n",
    "            query = input(\"\\n Query (type 'quit' to exit): \").strip()\n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                break \n",
    "            if not query:\n",
    "                continue\n",
    "            print(\"\\nProcessing...\")\n",
    "            start_time = time.time()\n",
    "            results = await self.reasoner.process_query(query)\n",
    "            processing_time = time.time() - start_time\n",
    "            self._display_results(results, processing_time)\n",
    "    def _display_results(self, results: Dict[str, Any], processing_time: float):\n",
    "        print(\"\\nResults:\")        \n",
    "        if 'error' in results:\n",
    "            print(f\"Error: {results['error']}\")\n",
    "            return\n",
    "        print(f\"Processing Time: {processing_time:.2f} seconds.\")\n",
    "        if 'scraping' in results.get('steps', {}):\n",
    "            scraping = results['steps']['scraping']\n",
    "            print(f\"Pages Scraped: {scraping['num_pages']}\")\n",
    "        if 'embedding' in results.get('steps', {}):\n",
    "            embedding = results['steps']['embedding']\n",
    "            print(f\"Text Chunks: {embedding['num_chunks']}\")\n",
    "        if 'retrieval' in results.get('steps', {}):\n",
    "            retrieval = results['steps']['retrieval']\n",
    "            print(f\"Retrieved Chunks: {retrieval['num_retrieved']}\")\n",
    "            if retrieval['scores']:\n",
    "                print(f\"Similarity Scores: {[f'{s:.3f}' for s in retrieval['scores'][:3]]}\")\n",
    "        print(\"\\nFinal Answer:\")\n",
    "        print(results.get('final_answer', 'No answer generated'))\n",
    "        print(\"\\n\")\n",
    "def system_health_check():\n",
    "    try:\n",
    "        import os\n",
    "        os.makedirs(config.cache_dir, exist_ok=True)\n",
    "        os.makedirs(config.persist_directory, exist_ok=True)        \n",
    "        import torch\n",
    "        import chromadb\n",
    "        import ollama\n",
    "        print(\"Starting...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"System health check failed: {e}.\")\n",
    "        return False\n",
    "def debug_vector_search(query: str):\n",
    "    print(f\"Debug: Testing vector search with query: {query}.\")\n",
    "async def main():\n",
    "    import sys    \n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"debug\":\n",
    "        print(\"Debug Mode\")\n",
    "        debug_vector_search(\"What is machine learning?\")\n",
    "    else:\n",
    "        try:\n",
    "            health_check_result = system_health_check()\n",
    "            if health_check_result:\n",
    "                print(\"\\nExecution Options:\")\n",
    "                print(\"1. Interactive Mode\")\n",
    "                print(\"2. Exit\")\n",
    "                try:\n",
    "                    choice = input(\"\\nSelect mode (1-2): \").strip()\n",
    "                    print(f\"Selected choice: {choice}\")\n",
    "                    if choice == \"1\":\n",
    "                        system = RAGSystem()\n",
    "                        print(\"Starting interactive mode...\")\n",
    "                        await system.run_interactive()\n",
    "                    elif choice == \"2\":\n",
    "                        print(\"\\nThe system ends.\")\n",
    "                        return\n",
    "                    else:\n",
    "                        print(\"Invalid choice. Please select 1 or 2.\")\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\n\\nGoodbye.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nUnexpected error in main(): {e}.\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                print(\"\\nSystem health check failed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during system health check: {e}.\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    import sys        \n",
    "    try:\n",
    "        get_ipython()        \n",
    "    except NameError:\n",
    "        try:\n",
    "            asyncio.run(main())\n",
    "        except Exception as e:\n",
    "            print(f\"Error in script execution: {e}.\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
